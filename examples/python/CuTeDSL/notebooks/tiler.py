# elementwise add with tiler
import torch
import cutlass
import cutlass.cute as cute
from cutlass.cute.runtime import from_dlpack
from typing import List
from functools import partial

# Basic Kernel Implementation
# ---------------------
# This is our first implementation of the elementwise add kernel.
# It follows a simple 1:1 mapping between threads and tensor elements.


@cute.kernel
def naive_elementwise_add_kernel(
    gA: cute.Tensor,  # Input tensor A
    gB: cute.Tensor,  # Input tensor B
    gC: cute.Tensor,  # Output tensor C = A + B
):
    # Step 1: Get thread indices
    # ------------------------
    # CUDA threads are organized in a 3D grid of thread blocks
    # Here we only use the x-dimension for simplicity
    tidx, _, _ = cute.arch.thread_idx()  # Thread index within block (0 to bdim-1)
    bidx, _, _ = cute.arch.block_idx()  # Block index in grid (0 to grid_dim-1)
    bdim, _, _ = cute.arch.block_dim()  # Number of threads per block

    # Calculate global thread index
    # This gives each thread a unique ID across all blocks
    thread_idx = bidx * bdim + tidx  # Global thread ID

    # Step 2: Map thread index to tensor coordinates
    # -------------------------------------------
    # Each thread will process one element of the input tensors
    m, n = gA.shape  # Get tensor dimensions (M rows × N columns)

    # Convert linear thread index to 2D coordinates:
    # - ni: column index (0 to n-1)
    # - mi: row index (0 to m-1)
    ni = thread_idx % n  # Column index (faster varying dimension)
    mi = thread_idx // n  # Row index (slower varying dimension)

    # Step 3: Load and process data
    # ---------------------------
    # Load values from input tensors
    # The tensor layout automatically handles the conversion from
    # logical indices (mi, ni) to physical memory addresses
    a_val = gA[mi, ni]  # Load element from tensor A
    b_val = gB[mi, ni]  # Load element from tensor B

    # Step 4: Store result
    # ------------------
    # Write the sum back to the output tensor
    gC[mi, ni] = a_val + b_val

@cute.jit  # Just-in-time compilation decorator
def naive_elementwise_add(
    mA: cute.Tensor,  # Input tensor A
    mB: cute.Tensor,  # Input tensor B
    mC: cute.Tensor,  # Output tensor C
):
    # Configure kernel launch parameters
    # --------------------------------
    # Choose number of threads per block
    # 256 is a common choice as it:
    # - Allows good occupancy on most GPUs
    # - Is a multiple of 32 (warp size)
    # - Provides enough threads for latency hiding
    num_threads_per_block = 256

    # Get input dimensions
    m, n = mA.shape  # Matrix dimensions (M rows × N columns)

    # Create kernel instance
    kernel = naive_elementwise_add_kernel(mA, mB, mC)

    # Launch kernel with calculated grid dimensions
    # -------------------------------------------
    # Grid size calculation:
    # - Total elements: m * n
    # - Blocks needed: ceil(total_elements / threads_per_block)
    # - Using integer division here assumes m * n is multiple of threads_per_block
    kernel.launch(
        grid=((m * n) // num_threads_per_block, 1, 1),  # Number of blocks in x,y,z
        block=(num_threads_per_block, 1, 1),  # Threads per block in x,y,z
    )

# Test Setup
# ----------
# Define test dimensions
def run_naive(M, N):
    
    # Create test data on GPU
    # ----------------------
    # Using float16 (half precision) for:
    # - Reduced memory bandwidth requirements
    # - Better performance on modern GPUs
    a = torch.randn(M, N, device="cuda", dtype=torch.float16)  # Random input A
    b = torch.randn(M, N, device="cuda", dtype=torch.float16)  # Random input B
    c = torch.zeros(M, N, device="cuda", dtype=torch.float16)  # Output buffer

    # Calculate total elements for bandwidth calculations
    num_elements = sum([a.numel(), b.numel(), c.numel()])

    # Convert PyTorch tensors to CuTe tensors
    # -------------------------------------
    # from_dlpack creates CuTe tensor views of PyTorch tensors
    # assumed_align=16 ensures proper memory alignment for vectorized access
    a_ = from_dlpack(a, assumed_align=16)  # CuTe tensor A
    b_ = from_dlpack(b, assumed_align=16)  # CuTe tensor B
    c_ = from_dlpack(c, assumed_align=16)  # CuTe tensor C

    # Compile the kernel for the specific input types
    naive_compiled_func = cute.compile(naive_elementwise_add, a_, b_, c_)

    # Run the kernel
    naive_compiled_func(mA=a_, mB=b_, mC=c_)
    print("Naive elementwise add verification:")
    torch.testing.assert_close(c, a + b)
    print("Naive elementwise add verification passed")
    benchmark(naive_compiled_func, a_, b_, c_, M, N)


'''
@cute.kernel
def naive_elementwise_add_kernel(
    gA: cute.Tensor,
    gB: cute.Tensor,
    gC: cute.Tensor,
):
    tidx, _, _ = cute.arch.thread_idx()
    bidx, _, _ = cute.arch.block_idx()
    bdim, _, _ = cute.arch.block_dim()

    # calc global thread index
    gtid = bidx * bdim + tidx
    # map to tensor coords
    m, n = gA.shape
    # convert linear index to 2D coords
    ni = gtid % n   # column index, faster varying dimension
    mi = gtid // n  # row index, slower varying dimension

    # load values from input tensors
    a_val = gA[mi, ni]
    b_val = gB[mi, ni]

    # store result to output tensor
    gC[mi, ni] = a_val + b_val

# host code
@cute.jit
def naive_elementwise_add(
    mA: cute.Tensor,
    mB: cute.Tensor,
    mC: cute.Tensor,
):
    num_threads_per_block = 256
    m, n = mA.shape
    # kernel instance
    kernel = naive_elementwise_add_kernel(mA, mB, mC)

    kernel.launch(
        grid=((m * n) // num_threads_per_block, 1, 1), # number of blocks in x,y,z
        block=(num_threads_per_block, 1, 1)
        ) # threads per block in x,y,z

if __name__ == "__main__":
    M, N = 16384, 8192
    a = torch.randn(M, N, device="cuda", dtype=torch.bfloat16)
    b = torch.randn(M, N, device="cuda", dtype=torch.bfloat16)
    c = torch.zeros(M, N, device="cuda", dtype=torch.bfloat16)

    num_elements = sum([a.numel(), b.numel(), c.numel()])
    a_ = from_dlpack(a, assumed_align=16)
    b_ = from_dlpack(b, assumed_align=16)
    c_ = from_dlpack(c, assumed_align=16)


    naive_ = cute.compile(naive_elementwise_add, a_, b_, c_)
    naive_(a_, b_, c_)
    torch.testing.assert_close(c, a + b)
'''

def benchmark(callable, a_, b_, c_, M, N):
    print(f"Benchmarking {callable} with M={M}, N={N}...\n")
    avg_time_us = cute.testing.benchmark(
        callable, 
        kernel_arguments = cute.testing.JitArguments(a_, b_,c_),
        warmup_iterations = 5,
        iterations = 100,
    )

    # calculate metrics
    dtype = a_.element_type
    num_elements = 3 * (M * N)

    # calculate total bytes transferred
    # 2 reads (A and B) + 1 write (C)
    # each element is dtype.width bits
    bytes_per_element = dtype.width // 8
    total_bytes = num_elements * bytes_per_element

    # calculate achieved bandwidth
    achieved_bandwidth = total_bytes / (avg_time_us * 1000) # GB/s

    # calculate achieved throughput in TB/s
    achieved_throughput_tb = achieved_bandwidth / 1000  # Convert GB/s to TB/s
    
    # print results
    print(f"\nPerformance Metrics:")
    print(f"-------------------")
    print(f"Kernel execution time: {avg_time_us / 1000:.4f} ms")
    print(f"Memory throughput: {achieved_bandwidth:.2f} GB/s")
    print(f"Memory throughput: {achieved_throughput_tb:.4f} TB/s")


# ====== vectorized elementwise add ======
@cute.kernel
def vectorized_elementwise_add_kernel(
    gA: cute.Tensor,
    gB: cute.Tensor,
    gC: cute.Tensor,
):
    tidx, _, _ = cute.arch.thread_idx()
    bidx, _, _ = cute.arch.block_idx()
    bdim, _, _ = cute.arch.block_dim()

    # calc global thread index
    thread_idx = bidx * bdim + tidx
    m, n = gA.shape[1]
    ni = thread_idx % n
    mi = thread_idx // n

    # map thread index to logical index in vector units
    # we extract a (1,4) sub-tensor from gA, gB and gC
    a_val = gA[(None, (mi, ni))].load()
    b_val = gB[(None, (mi, ni))].load()
    gC[(None, (mi, ni))] = a_val + b_val

@cute.jit
def vectorized_elementwise_add(mA: cute.Tensor, mB: cute.Tensor, mC: cute.Tensor):
    threads_per_block = 256

    gA = cute.zipped_divide(mA, (1, 8))
    gB = cute.zipped_divide(mB, (1, 8))
    gC = cute.zipped_divide(mC, (1, 8))

    print("[DSL INFO] Tiled Tensors:")
    print(f"[DSL INFO]   gA = {gA}")
    print(f"[DSL INFO]   gB = {gB}")
    print(f"[DSL INFO]   gC = {gC}")

    vectorized_elementwise_add_kernel(gA, gB, gC).launch(
        grid=(cute.size(gC, mode=[1]) // threads_per_block, 1, 1),
        block=(threads_per_block, 1, 1),
    )


def run_vectorized(M, N):
    a = torch.randn(M, N, device="cuda", dtype=torch.bfloat16)
    b = torch.randn(M, N, device="cuda", dtype=torch.bfloat16)
    c = torch.zeros(M, N, device="cuda", dtype=torch.bfloat16)

    num_elements = sum([a.numel(), b.numel(), c.numel()])

    a_ = from_dlpack(a, assumed_align=16)
    b_ = from_dlpack(b, assumed_align=16)
    c_ = from_dlpack(c, assumed_align=16)

    vectorized_ = cute.compile(vectorized_elementwise_add, a_, b_, c_)
    vectorized_(mA=a_, mB=b_, mC=c_)
    
    torch.testing.assert_close(c, a + b)
    print("Vectorized elementwise add verification passed\n")
    benchmark(vectorized_, a_, b_, c_, M, N)

@cute.kernel
def elementwise_add_kernel(
    gA: cute.Tensor, gB: cute.Tensor, gC: cute.Tensor, tv_layout: cute.Layout
):
    tidx, _, _ = cute.arch.thread_idx()
    bidx, _, _ = cute.arch.block_idx()

    # --------------------------------
    # slice for thread-block level view
    # --------------------------------
    blk_coord = ((None, None), bidx)

    # logical coord -> address
    blkA = gA[blk_coord]  # (TileM, TileN) -> physical address
    blkB = gB[blk_coord]  # (TileM, TileN) -> physical address
    blkC = gC[blk_coord]  # (TileM, TileN) -> physical address

    # --------------------------------
    # compose for thread-index & value-index to physical mapping
    # --------------------------------
    # blockA:    (TileM, TileN) -> physical address
    # tv_layout: (tid, vid)     -> (TileM, TileN)
    # tidfrgA = blkA o tv_layout
    # tidfrgA:   (tid, vid) -> physical address
    tidfrgA = cute.composition(blkA, tv_layout)
    tidfrgB = cute.composition(blkB, tv_layout)
    tidfrgC = cute.composition(blkC, tv_layout)

    # print("Composed with TV layout:")
    #print(f"  tidfrgA: {tidfrgA.type}")

    # --------------------------------
    # slice for thread-level view
    # --------------------------------
    # `None` represent slice of the entire per-thread data
    thr_coord = (tidx, None)
    # thr_coord = (tidx, cute.repeat_like(None, gA.shape[1]))

    # slice for threads: vid -> address
    thrA = tidfrgA[thr_coord]  # (V) -> physical address
    thrB = tidfrgB[thr_coord]  # (V) -> physical address
    thrC = tidfrgC[thr_coord]  # (V) -> physical address

    thrC[None] = thrA.load() + thrB.load()

@cute.jit
def tv_elementwise_add(
    mA: cute.Tensor,
    mB: cute.Tensor,
    mC: cute.Tensor,
):
    # mA layout: (M, N):(N, 1)
    # TV layout map thread & value index to (64, 512) logical tile
    #  - contiguous thread index maps to mode-1 because input layout is contiguous on
    #     mode-1 for coalesced load-store
    #  - each thread load contiguous 16 bytes each row and load 16 rows
    coalesced_ldst_bytes = 16

    # Compile time validation: expect same element type for all input tensors
    assert all(t.element_type == mA.element_type for t in [mA, mB, mC])
    dtype = mA.element_type

    thr_layout = cute.make_ordered_layout((4, 64), order=(1, 0))
    val_layout = cute.make_ordered_layout((16, coalesced_ldst_bytes), order=(1, 0))
    val_layout = cute.recast_layout(dtype.width, 8, val_layout)
    tiler_mn, tv_layout = cute.make_layout_tv(thr_layout, val_layout)

    print(f"[DSL INFO] Tiler: {tiler_mn}")
    print(f"[DSL INFO] TV Layout: {tv_layout}")

    gA = cute.zipped_divide(mA, tiler_mn)  # ((TileM, TileN), (RestM, RestN))
    gB = cute.zipped_divide(mB, tiler_mn)  # ((TileM, TileN), (RestM, RestN))
    gC = cute.zipped_divide(mC, tiler_mn)  # ((TileM, TileN), (RestM, RestN))

    print("Tiled Input Tensors:")
    print("[DSL INFO] Tiled Tensors:")
    print(f"[DSL INFO]   gA = {gA.type}")
    print(f"[DSL INFO]   gB = {gB.type}")
    print(f"[DSL INFO]   gC = {gC.type}")

    # Launch the kernel asynchronously
    # Async token(s) can also be specified as dependencies
    # Launch the kernel asynchronously
    # Async token(s) can also be specified as dependencies
    elementwise_add_kernel(gA, gB, gC, tv_layout).launch(
        grid=[cute.size(gC, mode=[1]), 1, 1],
        block=[cute.size(tv_layout, mode=[0]), 1, 1],
    )
    
    
def run_tv(M, N):
    a = torch.randn(M, N, device="cuda", dtype=torch.float16)
    b = torch.randn(M, N, device="cuda", dtype=torch.float16)
    c = torch.zeros(M, N, device="cuda", dtype=torch.float16)

    a_ = from_dlpack(a, assumed_align=16)
    b_ = from_dlpack(b, assumed_align=16)
    c_ = from_dlpack(c, assumed_align=16)

    tv_add_kernel_compiled = cute.compile(tv_elementwise_add, a_, b_, c_)
    tv_add_kernel_compiled(mA=a_, mB=b_, mC=c_)
    

    # verify correctness
    torch.testing.assert_close(c, a + b)
    print("TV elementwise add verification passed\n")
    benchmark(tv_add_kernel_compiled, a_, b_, c_, M, N)

if __name__ == "__main__":
    M, N = 16384, 8192
    print(f"\nRunning naive elementwise add with M={M}, N={N}...\n")
    run_naive(M, N)

    print(f"\nRunning vectorized elementwise add with M={M}, N={N}...\n")
    run_vectorized(M, N)

    print(f"\nRunning TV elementwise add with M={M}, N={N}...\n")
    run_tv(M, N)
    

    